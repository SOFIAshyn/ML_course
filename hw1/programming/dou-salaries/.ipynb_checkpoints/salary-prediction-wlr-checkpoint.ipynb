{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Передбачення зарплат на IT-ринку України"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У цьому завданні ви працюватимете з реальними даними з [зарплатного опитування DOU.ua за травень 2016р](https://dou.ua/lenta/articles/salary-report-may-june-2016/). Ви реалізуєте зважену лінійну регресію, яка передбачає зарплати Java-інженерів, та навчите свою модель за допомогою градієнтного спуску.\n",
    "\n",
    "Заповніть пропущений код в розділі «Моделювання» (позначено коментарями) та запустіть розділ «Тестування», щоб перевірити його правильність."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ipython_unittest extension is already loaded. To reload it, use:\n",
      "  %reload_ext ipython_unittest\n"
     ]
    }
   ],
   "source": [
    "%load_ext ipython_unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Підготовка даних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salaries = pd.read_csv(\"data/2016_may_final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оберемо тільки Java-інженерів з-поміж усіх респондентів."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_java = pd.DataFrame(df_salaries[(df_salaries[\"Язык.программирования\"] == \"Java\") &\n",
    "                                   (df_salaries[\"cls\"] == \"DEV\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перейменуємо деякі колонки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_java.rename(\n",
    "    columns={\n",
    "        \"exp\": \"TotalExperience\",\n",
    "        \"loc\": \"Location\"\n",
    "    },\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Закодуємо рівень англійської мови числами від 1 (найнижчий) до 5 (найвищий):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_java[\"EnglishLevel\"] = df_java[\"Уровень.английского\"].map({\n",
    "    \"элементарный\": 1,\n",
    "    \"ниже среднего\": 2,\n",
    "    \"средний\": 3,\n",
    "    \"выше среднего\": 4,\n",
    "    \"продвинутый\": 5\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Закодуємо колонку Location (найбільші IT-міста або \"other\") за допомогою one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_columns = [\n",
    "    \"LocationOther\",\n",
    "    \"LocationDnipro\",\n",
    "    \"LocationKyiv\",\n",
    "    \"LocationLviv\",\n",
    "    \"LocationOdesa\",\n",
    "    \"LocationKharkiv\"\n",
    "]\n",
    "df_java[city_columns] = pd.get_dummies(df_java[\"Location\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>Должность</th>\n",
       "      <th>Язык.программирования</th>\n",
       "      <th>Специализация</th>\n",
       "      <th>Общий.опыт.работы</th>\n",
       "      <th>Опыт.работы.на.текущем.месте</th>\n",
       "      <th>Зарплата.в.месяц</th>\n",
       "      <th>Изменение.зарплаты.за.12.месяцев</th>\n",
       "      <th>Город</th>\n",
       "      <th>Размер.компании</th>\n",
       "      <th>...</th>\n",
       "      <th>salary</th>\n",
       "      <th>Валюта</th>\n",
       "      <th>cls</th>\n",
       "      <th>EnglishLevel</th>\n",
       "      <th>LocationOther</th>\n",
       "      <th>LocationDnipro</th>\n",
       "      <th>LocationKyiv</th>\n",
       "      <th>LocationLviv</th>\n",
       "      <th>LocationOdesa</th>\n",
       "      <th>LocationKharkiv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Junior Software Engineer</td>\n",
       "      <td>Java</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>500</td>\n",
       "      <td>100</td>\n",
       "      <td>Черновцы</td>\n",
       "      <td>до 50 человек</td>\n",
       "      <td>...</td>\n",
       "      <td>500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DEV</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>Java</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1600</td>\n",
       "      <td>-400</td>\n",
       "      <td>Киев</td>\n",
       "      <td>до 50 человек</td>\n",
       "      <td>...</td>\n",
       "      <td>1600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DEV</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Junior Software Engineer</td>\n",
       "      <td>Java</td>\n",
       "      <td>NaN</td>\n",
       "      <td>меньше 3 месяцев</td>\n",
       "      <td>меньше 3 месяцев</td>\n",
       "      <td>600</td>\n",
       "      <td>0</td>\n",
       "      <td>Киев</td>\n",
       "      <td>свыше 1000 человек</td>\n",
       "      <td>...</td>\n",
       "      <td>600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DEV</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Senior Software Engineer</td>\n",
       "      <td>Java</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3400</td>\n",
       "      <td>1000</td>\n",
       "      <td>Киев</td>\n",
       "      <td>до 200 человек</td>\n",
       "      <td>...</td>\n",
       "      <td>3400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DEV</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>Senior Software Engineer</td>\n",
       "      <td>Java</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2880</td>\n",
       "      <td>850</td>\n",
       "      <td>Киев</td>\n",
       "      <td>до 200 человек</td>\n",
       "      <td>...</td>\n",
       "      <td>2880</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DEV</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     N                 Должность Язык.программирования Специализация  \\\n",
       "5    6  Junior Software Engineer                  Java           NaN   \n",
       "7    8         Software Engineer                  Java           NaN   \n",
       "17  18  Junior Software Engineer                  Java           NaN   \n",
       "27  28  Senior Software Engineer                  Java           NaN   \n",
       "28  29  Senior Software Engineer                  Java           NaN   \n",
       "\n",
       "   Общий.опыт.работы Опыт.работы.на.текущем.месте  Зарплата.в.месяц  \\\n",
       "5                0.5                          0.5               500   \n",
       "7                  5                         0.25              1600   \n",
       "17  меньше 3 месяцев             меньше 3 месяцев               600   \n",
       "27                 4                            1              3400   \n",
       "28                 6                          0.5              2880   \n",
       "\n",
       "    Изменение.зарплаты.за.12.месяцев     Город     Размер.компании  ...  \\\n",
       "5                                100  Черновцы       до 50 человек  ...   \n",
       "7                               -400      Киев       до 50 человек  ...   \n",
       "17                                 0      Киев  свыше 1000 человек  ...   \n",
       "27                              1000      Киев      до 200 человек  ...   \n",
       "28                               850      Киев      до 200 человек  ...   \n",
       "\n",
       "   salary Валюта  cls EnglishLevel LocationOther  LocationDnipro LocationKyiv  \\\n",
       "5     500    NaN  DEV            3             1               0            0   \n",
       "7    1600    NaN  DEV            1             0               0            1   \n",
       "17    600    NaN  DEV            3             0               0            1   \n",
       "27   3400    NaN  DEV            3             0               0            1   \n",
       "28   2880    NaN  DEV            4             0               0            1   \n",
       "\n",
       "   LocationLviv LocationOdesa LocationKharkiv  \n",
       "5             0             0               0  \n",
       "7             0             0               0  \n",
       "17            0             0               0  \n",
       "27            0             0               0  \n",
       "28            0             0               0  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_java.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Відберемо такі ознаки:\n",
    "\n",
    "* Загальна кількість років досвіду\n",
    "* Рівень англійської мови\n",
    "* Місто"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\"TotalExperience\", \"EnglishLevel\"] + city_columns\n",
    "df_X = df_java[feature_columns]\n",
    "df_y = df_java[[\"salary\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (929, 8)\n"
     ]
    }
   ],
   "source": [
    "print(\"X shape:\", df_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TotalExperience</th>\n",
       "      <th>EnglishLevel</th>\n",
       "      <th>LocationOther</th>\n",
       "      <th>LocationDnipro</th>\n",
       "      <th>LocationKyiv</th>\n",
       "      <th>LocationLviv</th>\n",
       "      <th>LocationOdesa</th>\n",
       "      <th>LocationKharkiv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    TotalExperience  EnglishLevel  LocationOther  LocationDnipro  \\\n",
       "5               0.5             3              1               0   \n",
       "7               5.0             1              0               0   \n",
       "17              0.0             3              0               0   \n",
       "27              4.0             3              0               0   \n",
       "28              6.0             4              0               0   \n",
       "39              3.0             4              0               1   \n",
       "46              2.0             4              0               0   \n",
       "49              3.0             3              0               0   \n",
       "59              2.0             3              0               0   \n",
       "89              1.0             5              0               0   \n",
       "\n",
       "    LocationKyiv  LocationLviv  LocationOdesa  LocationKharkiv  \n",
       "5              0             0              0                0  \n",
       "7              1             0              0                0  \n",
       "17             1             0              0                0  \n",
       "27             1             0              0                0  \n",
       "28             1             0              0                0  \n",
       "39             0             0              0                0  \n",
       "46             0             0              0                1  \n",
       "49             1             0              0                0  \n",
       "59             0             0              0                1  \n",
       "89             1             0              0                0  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    salary\n",
       "5      500\n",
       "7     1600\n",
       "17     600\n",
       "27    3400\n",
       "28    2880\n",
       "39    1425\n",
       "46    1700\n",
       "49    1800\n",
       "59    1235\n",
       "89    1200"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Розділимо вибірку на навчальну та тестову:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_size = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_assignment = np.random.uniform(size=len(df_X))\n",
    "\n",
    "X_train = df_X[dataset_assignment <= training_set_size].values\n",
    "y_train = df_y[dataset_assignment <= training_set_size].values.flatten()\n",
    "\n",
    "X_test = df_X[dataset_assignment > training_set_size].values\n",
    "y_test = df_y[dataset_assignment > training_set_size].values.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Щоб градієнтний спуск швидше збігався, нормалізуємо навчальну вибірку так, щоб кожна ознака мала $\\mu = 0, \\sigma = 1$:\n",
    "\n",
    "$ x' = \\frac{x - \\bar{x}}{\\sigma}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_means = np.average(X_train, axis=0)\n",
    "feature_sigmas = np.std(X_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = (X_train - feature_means) / feature_sigmas\n",
    "X_test = (X_test - feature_means) / feature_sigmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Додаємо уявну ознаку $x_0 = 1$ (intercept term)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not np.all(X_train[:, 0] == 1):\n",
    "    X_train = np.insert(X_train, 0, values=1, axis=1)\n",
    "    \n",
    "if not np.all(X_test[:, 0] == 1):\n",
    "    X_test = np.insert(X_test, 0, values=1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train:  (741, 9)\n",
      "y train:  (741,)\n",
      "\n",
      "X test:   (188, 9)\n",
      "y test:   (188,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X train: \", X_train.shape)\n",
    "print(\"y train: \", y_train.shape)\n",
    "print()\n",
    "print(\"X test:  \", X_test.shape)\n",
    "print(\"y test:  \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Моделювання"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реалізуйте функцію гіпотези лінійної регресії в матричній формі."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_linear(theta, X):\n",
    "    # Compute the hypothesis function for linear regression.\n",
    "    return np.dot(X, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реалізуйте функцію зважування всіх навчальних прикладів $x^{(i)}$, якщо нам дана точка передбачення $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If x_i is a scalar:\n",
    "$$ w^i = exp(\\frac{-(x^i - x)^2}{2\\tau^2}) $$\n",
    "##### If x_i is a vector:\n",
    "$$ w^i = exp(\\frac{-(x^i - x)^T \\cdot (x^i - x)}{2\\tau^2}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example_weights(X, x_pred, tau):\n",
    "    # Compute the weight for each example, given the\n",
    "    # prediction point (x_pred).\n",
    "    weights = np.array([])\n",
    "    for i in range(X.shape[0]):\n",
    "        val = np.exp(-np.dot((X[i] - x_pred).T, (X[i] - x_pred))/(2*tau**2))\n",
    "        weights = np.insert(weights, i, val)\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реалізуйте функцію втрат зваженої лінійної регресії. Подумайте, як обчислити цей вираз відразу в матричному вигляді."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ J(\\theta) = (X\\theta - \\hat{y})^T W(X\\theta - \\hat{y}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_W(weights):\n",
    "    w_size = weights.shape[0]\n",
    "    W = np.zeros((w_size, w_size))\n",
    "    for i in range(w_size):\n",
    "        W[i, i] = weights[i]\n",
    "    return W\n",
    "\n",
    "def cost_function(theta, X, y, weights):\n",
    "    # Given the currently learned model weights (theta),\n",
    "    # compute the overall loss on the training set (X),\n",
    "    # taking the weights into account.\n",
    "    m = len(X)\n",
    "    W = create_W(weights)\n",
    "    \n",
    "    br_2 = (np.dot(X, theta.T) - y.T)\n",
    "    br_1 = (np.dot(X, theta.T) - y.T).T # br_2 TODO: hh\n",
    "    \n",
    "    return (1/(2*m)) * np.dot(np.dot(br_1, W), br_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реалізуйте обчислення градієнта функції втрат зваженої лінійної регресії. Подумайте, як обчислити цей вираз відразу в матричному вигляді."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial}{\\partial \\theta_j} J(\\theta) = \\dfrac{1}{m}\\sum_{i=1}^{m}\\left( h_\\theta(x^{(i)}) - y^{(i)} \\right) w^{(i)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function_gradient(theta, X, y, weights):\n",
    "    # Given the currently learned model weights (theta),\n",
    "    # compute the gradient of the cost function on the\n",
    "    # training set (X), taking the weights into account.\n",
    "    m = len(X)\n",
    "    W = create_W(weights)\n",
    "    lin_reg_predictor = predict_linear(theta, X)\n",
    "\n",
    "    return (1/m) * np.dot(np.dot(X.T, W), (lin_reg_predictor - y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реалізуйте один крок градієнтного спуску."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model_weights(theta, learning_rate, cost_gradient):\n",
    "    # Given the learning rate and the gradient of the\n",
    "    # cost function, take one gradient descent step and\n",
    "    # return the updated vector theta.\n",
    "    theta = theta - learning_rate * cost_gradient\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Навчаємо модель:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, weights, loss_fun, grad_fun, learning_rate, convergence_threshold, max_iters, verbose=False):\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    losses = []\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        loss = loss_fun(theta, X, y, weights)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Iteration: {0:3} Loss: {1}\".format(i + 1, loss))\n",
    "\n",
    "        if len(losses) > 2 and np.abs(losses[-1] - losses[-2]) <= convergence_threshold:\n",
    "            break\n",
    "        \n",
    "        grad = grad_fun(theta, X, y, weights)\n",
    "        theta = update_model_weights(theta, learning_rate, grad)\n",
    "        \n",
    "    return theta, np.array(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Передбачення нових даних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_weighted_linear(X, y, x_pred, verbose=False):\n",
    "    weights = get_example_weights(X, x_pred, tau=0.1)\n",
    "    theta, losses = gradient_descent(\n",
    "        X,\n",
    "        y,\n",
    "        weights,\n",
    "        loss_fun=cost_function,\n",
    "        grad_fun=cost_function_gradient,\n",
    "        learning_rate=0.005,\n",
    "        convergence_threshold=0.0001,\n",
    "        max_iters=500,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    return predict_linear(theta, x_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = X_train[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:   1 Loss: 2051.275762424122\n",
      "Iteration:   2 Loss: 2050.49351072097\n",
      "Iteration:   3 Loss: 2049.711566651771\n",
      "Iteration:   4 Loss: 2048.9299300955427\n",
      "Iteration:   5 Loss: 2048.1486009313508\n",
      "Iteration:   6 Loss: 2047.3675790383077\n",
      "Iteration:   7 Loss: 2046.586864295574\n",
      "Iteration:   8 Loss: 2045.8064565823574\n",
      "Iteration:   9 Loss: 2045.0263557779133\n",
      "Iteration:  10 Loss: 2044.2465617615442\n",
      "Iteration:  11 Loss: 2043.4670744126004\n",
      "Iteration:  12 Loss: 2042.6878936104802\n",
      "Iteration:  13 Loss: 2041.909019234629\n",
      "Iteration:  14 Loss: 2041.1304511645385\n",
      "Iteration:  15 Loss: 2040.3521892797496\n",
      "Iteration:  16 Loss: 2039.5742334598485\n",
      "Iteration:  17 Loss: 2038.7965835844707\n",
      "Iteration:  18 Loss: 2038.0192395332986\n",
      "Iteration:  19 Loss: 2037.2422011860604\n",
      "Iteration:  20 Loss: 2036.4654684225336\n",
      "Iteration:  21 Loss: 2035.689041122542\n",
      "Iteration:  22 Loss: 2034.9129191659574\n",
      "Iteration:  23 Loss: 2034.1371024326966\n",
      "Iteration:  24 Loss: 2033.3615908027275\n",
      "Iteration:  25 Loss: 2032.586384156061\n",
      "Iteration:  26 Loss: 2031.811482372758\n",
      "Iteration:  27 Loss: 2031.0368853329257\n",
      "Iteration:  28 Loss: 2030.2625929167186\n",
      "Iteration:  29 Loss: 2029.488605004338\n",
      "Iteration:  30 Loss: 2028.7149214760327\n",
      "Iteration:  31 Loss: 2027.941542212098\n",
      "Iteration:  32 Loss: 2027.1684670928776\n",
      "Iteration:  33 Loss: 2026.3956959987606\n",
      "Iteration:  34 Loss: 2025.623228810184\n",
      "Iteration:  35 Loss: 2024.8510654076317\n",
      "Iteration:  36 Loss: 2024.0792056716343\n",
      "Iteration:  37 Loss: 2023.3076494827715\n",
      "Iteration:  38 Loss: 2022.5363967216654\n",
      "Iteration:  39 Loss: 2021.7654472689892\n",
      "Iteration:  40 Loss: 2020.9948010054623\n",
      "Iteration:  41 Loss: 2020.2244578118493\n",
      "Iteration:  42 Loss: 2019.454417568963\n",
      "Iteration:  43 Loss: 2018.684680157663\n",
      "Iteration:  44 Loss: 2017.9152454588557\n",
      "Iteration:  45 Loss: 2017.1461133534935\n",
      "Iteration:  46 Loss: 2016.377283722577\n",
      "Iteration:  47 Loss: 2015.608756447153\n",
      "Iteration:  48 Loss: 2014.8405314083145\n",
      "Iteration:  49 Loss: 2014.0726084872013\n",
      "Iteration:  50 Loss: 2013.3049875650017\n",
      "Iteration:  51 Loss: 2012.5376685229487\n",
      "Iteration:  52 Loss: 2011.770651242323\n",
      "Iteration:  53 Loss: 2011.0039356044508\n",
      "Iteration:  54 Loss: 2010.2375214907074\n",
      "Iteration:  55 Loss: 2009.471408782512\n",
      "Iteration:  56 Loss: 2008.7055973613321\n",
      "Iteration:  57 Loss: 2007.940087108682\n",
      "Iteration:  58 Loss: 2007.174877906121\n",
      "Iteration:  59 Loss: 2006.4099696352564\n",
      "Iteration:  60 Loss: 2005.6453621777416\n",
      "Iteration:  61 Loss: 2004.881055415277\n",
      "Iteration:  62 Loss: 2004.117049229608\n",
      "Iteration:  63 Loss: 2003.3533435025288\n",
      "Iteration:  64 Loss: 2002.5899381158783\n",
      "Iteration:  65 Loss: 2001.8268329515427\n",
      "Iteration:  66 Loss: 2001.0640278914534\n",
      "Iteration:  67 Loss: 2000.3015228175907\n",
      "Iteration:  68 Loss: 1999.5393176119794\n",
      "Iteration:  69 Loss: 1998.7774121566908\n",
      "Iteration:  70 Loss: 1998.015806333843\n",
      "Iteration:  71 Loss: 1997.2545000256005\n",
      "Iteration:  72 Loss: 1996.493493114174\n",
      "Iteration:  73 Loss: 1995.7327854818202\n",
      "Iteration:  74 Loss: 1994.9723770108433\n",
      "Iteration:  75 Loss: 1994.2122675835917\n",
      "Iteration:  76 Loss: 1993.452457082463\n",
      "Iteration:  77 Loss: 1992.6929453898981\n",
      "Iteration:  78 Loss: 1991.9337323883851\n",
      "Iteration:  79 Loss: 1991.1748179604597\n",
      "Iteration:  80 Loss: 1990.4162019887024\n",
      "Iteration:  81 Loss: 1989.657884355739\n",
      "Iteration:  82 Loss: 1988.8998649442444\n",
      "Iteration:  83 Loss: 1988.142143636937\n",
      "Iteration:  84 Loss: 1987.3847203165822\n",
      "Iteration:  85 Loss: 1986.6275948659913\n",
      "Iteration:  86 Loss: 1985.870767168022\n",
      "Iteration:  87 Loss: 1985.114237105578\n",
      "Iteration:  88 Loss: 1984.3580045616097\n",
      "Iteration:  89 Loss: 1983.6020694191118\n",
      "Iteration:  90 Loss: 1982.8464315611266\n",
      "Iteration:  91 Loss: 1982.0910908707417\n",
      "Iteration:  92 Loss: 1981.3360472310906\n",
      "Iteration:  93 Loss: 1980.581300525353\n",
      "Iteration:  94 Loss: 1979.8268506367556\n",
      "Iteration:  95 Loss: 1979.0726974485685\n",
      "Iteration:  96 Loss: 1978.3188408441101\n",
      "Iteration:  97 Loss: 1977.5652807067436\n",
      "Iteration:  98 Loss: 1976.8120169198767\n",
      "Iteration:  99 Loss: 1976.0590493669672\n",
      "Iteration: 100 Loss: 1975.306377931514\n",
      "Iteration: 101 Loss: 1974.5540024970646\n",
      "Iteration: 102 Loss: 1973.8019229472106\n",
      "Iteration: 103 Loss: 1973.0501391655914\n",
      "Iteration: 104 Loss: 1972.2986510358903\n",
      "Iteration: 105 Loss: 1971.547458441838\n",
      "Iteration: 106 Loss: 1970.7965612672092\n",
      "Iteration: 107 Loss: 1970.0459593958255\n",
      "Iteration: 108 Loss: 1969.295652711553\n",
      "Iteration: 109 Loss: 1968.5456410983056\n",
      "Iteration: 110 Loss: 1967.795924440041\n",
      "Iteration: 111 Loss: 1967.046502620762\n",
      "Iteration: 112 Loss: 1966.2973755245202\n",
      "Iteration: 113 Loss: 1965.548543035409\n",
      "Iteration: 114 Loss: 1964.8000050375692\n",
      "Iteration: 115 Loss: 1964.0517614151881\n",
      "Iteration: 116 Loss: 1963.3038120524964\n",
      "Iteration: 117 Loss: 1962.5561568337723\n",
      "Iteration: 118 Loss: 1961.8087956433378\n",
      "Iteration: 119 Loss: 1961.0617283655615\n",
      "Iteration: 120 Loss: 1960.3149548848576\n",
      "Iteration: 121 Loss: 1959.5684750856847\n",
      "Iteration: 122 Loss: 1958.8222888525481\n",
      "Iteration: 123 Loss: 1958.0763960699971\n",
      "Iteration: 124 Loss: 1957.3307966226284\n",
      "Iteration: 125 Loss: 1956.5854903950817\n",
      "Iteration: 126 Loss: 1955.8404772720435\n",
      "Iteration: 127 Loss: 1955.095757138246\n",
      "Iteration: 128 Loss: 1954.3513298784655\n",
      "Iteration: 129 Loss: 1953.6071953775247\n",
      "Iteration: 130 Loss: 1952.8633535202907\n",
      "Iteration: 131 Loss: 1952.119804191677\n",
      "Iteration: 132 Loss: 1951.3765472766413\n",
      "Iteration: 133 Loss: 1950.6335826601864\n",
      "Iteration: 134 Loss: 1949.8909102273615\n",
      "Iteration: 135 Loss: 1949.1485298632601\n",
      "Iteration: 136 Loss: 1948.406441453022\n",
      "Iteration: 137 Loss: 1947.6646448818292\n",
      "Iteration: 138 Loss: 1946.9231400349133\n",
      "Iteration: 139 Loss: 1946.1819267975475\n",
      "Iteration: 140 Loss: 1945.4410050550514\n",
      "Iteration: 141 Loss: 1944.7003746927903\n",
      "Iteration: 142 Loss: 1943.9600355961725\n",
      "Iteration: 143 Loss: 1943.219987650654\n",
      "Iteration: 144 Loss: 1942.4802307417353\n",
      "Iteration: 145 Loss: 1941.7407647549587\n",
      "Iteration: 146 Loss: 1941.0015895759166\n",
      "Iteration: 147 Loss: 1940.2627050902422\n",
      "Iteration: 148 Loss: 1939.5241111836162\n",
      "Iteration: 149 Loss: 1938.7858077417627\n",
      "Iteration: 150 Loss: 1938.0477946504527\n",
      "Iteration: 151 Loss: 1937.3100717954994\n",
      "Iteration: 152 Loss: 1936.5726390627628\n",
      "Iteration: 153 Loss: 1935.835496338148\n",
      "Iteration: 154 Loss: 1935.0986435076036\n",
      "Iteration: 155 Loss: 1934.3620804571242\n",
      "Iteration: 156 Loss: 1933.6258070727479\n",
      "Iteration: 157 Loss: 1932.88982324056\n",
      "Iteration: 158 Loss: 1932.1541288466885\n",
      "Iteration: 159 Loss: 1931.4187237773056\n",
      "Iteration: 160 Loss: 1930.683607918631\n",
      "Iteration: 161 Loss: 1929.9487811569272\n",
      "Iteration: 162 Loss: 1929.2142433785011\n",
      "Iteration: 163 Loss: 1928.4799944697065\n",
      "Iteration: 164 Loss: 1927.7460343169387\n",
      "Iteration: 165 Loss: 1927.0123628066415\n",
      "Iteration: 166 Loss: 1926.2789798252995\n",
      "Iteration: 167 Loss: 1925.5458852594443\n",
      "Iteration: 168 Loss: 1924.8130789956513\n",
      "Iteration: 169 Loss: 1924.0805609205408\n",
      "Iteration: 170 Loss: 1923.3483309207788\n",
      "Iteration: 171 Loss: 1922.6163888830731\n",
      "Iteration: 172 Loss: 1921.8847346941784\n",
      "Iteration: 173 Loss: 1921.153368240893\n",
      "Iteration: 174 Loss: 1920.4222894100608\n",
      "Iteration: 175 Loss: 1919.6914980885683\n",
      "Iteration: 176 Loss: 1918.9609941633473\n",
      "Iteration: 177 Loss: 1918.230777521375\n",
      "Iteration: 178 Loss: 1917.5008480496715\n",
      "Iteration: 179 Loss: 1916.7712056353037\n",
      "Iteration: 180 Loss: 1916.0418501653796\n",
      "Iteration: 181 Loss: 1915.3127815270543\n",
      "Iteration: 182 Loss: 1914.583999607526\n",
      "Iteration: 183 Loss: 1913.8555042940382\n",
      "Iteration: 184 Loss: 1913.1272954738772\n",
      "Iteration: 185 Loss: 1912.3993730343757\n",
      "Iteration: 186 Loss: 1911.671736862908\n",
      "Iteration: 187 Loss: 1910.9443868468954\n",
      "Iteration: 188 Loss: 1910.217322873802\n",
      "Iteration: 189 Loss: 1909.4905448311376\n",
      "Iteration: 190 Loss: 1908.764052606453\n",
      "Iteration: 191 Loss: 1908.0378460873474\n",
      "Iteration: 192 Loss: 1907.3119251614607\n",
      "Iteration: 193 Loss: 1906.5862897164793\n",
      "Iteration: 194 Loss: 1905.8609396401323\n",
      "Iteration: 195 Loss: 1905.1358748201944\n",
      "Iteration: 196 Loss: 1904.4110951444823\n",
      "Iteration: 197 Loss: 1903.6866005008594\n",
      "Iteration: 198 Loss: 1902.9623907772316\n",
      "Iteration: 199 Loss: 1902.2384658615485\n",
      "Iteration: 200 Loss: 1901.5148256418058\n",
      "Iteration: 201 Loss: 1900.791470006041\n",
      "Iteration: 202 Loss: 1900.0683988423366\n",
      "Iteration: 203 Loss: 1899.345612038819\n",
      "Iteration: 204 Loss: 1898.6231094836587\n",
      "Iteration: 205 Loss: 1897.9008910650705\n",
      "Iteration: 206 Loss: 1897.1789566713128\n",
      "Iteration: 207 Loss: 1896.4573061906872\n",
      "Iteration: 208 Loss: 1895.735939511541\n",
      "Iteration: 209 Loss: 1895.0148565222637\n",
      "Iteration: 210 Loss: 1894.2940571112888\n",
      "Iteration: 211 Loss: 1893.5735411670955\n",
      "Iteration: 212 Loss: 1892.8533085782053\n",
      "Iteration: 213 Loss: 1892.1333592331835\n",
      "Iteration: 214 Loss: 1891.4136930206396\n",
      "Iteration: 215 Loss: 1890.694309829227\n",
      "Iteration: 216 Loss: 1889.9752095476429\n",
      "Iteration: 217 Loss: 1889.2563920646278\n",
      "Iteration: 218 Loss: 1888.5378572689667\n",
      "Iteration: 219 Loss: 1887.8196050494876\n",
      "Iteration: 220 Loss: 1887.1016352950628\n",
      "Iteration: 221 Loss: 1886.3839478946077\n",
      "Iteration: 222 Loss: 1885.6665427370824\n",
      "Iteration: 223 Loss: 1884.9494197114886\n",
      "Iteration: 224 Loss: 1884.2325787068746\n",
      "Iteration: 225 Loss: 1883.5160196123302\n",
      "Iteration: 226 Loss: 1882.7997423169895\n",
      "Iteration: 227 Loss: 1882.08374671003\n",
      "Iteration: 228 Loss: 1881.3680326806732\n",
      "Iteration: 229 Loss: 1880.652600118183\n",
      "Iteration: 230 Loss: 1879.9374489118684\n",
      "Iteration: 231 Loss: 1879.222578951082\n",
      "Iteration: 232 Loss: 1878.5079901252172\n",
      "Iteration: 233 Loss: 1877.7936823237144\n",
      "Iteration: 234 Loss: 1877.0796554360552\n",
      "Iteration: 235 Loss: 1876.3659093517658\n",
      "Iteration: 236 Loss: 1875.6524439604152\n",
      "Iteration: 237 Loss: 1874.939259151616\n",
      "Iteration: 238 Loss: 1874.226354815025\n",
      "Iteration: 239 Loss: 1873.5137308403412\n",
      "Iteration: 240 Loss: 1872.8013871173068\n",
      "Iteration: 241 Loss: 1872.089323535709\n",
      "Iteration: 242 Loss: 1871.3775399853766\n",
      "Iteration: 243 Loss: 1870.6660363561825\n",
      "Iteration: 244 Loss: 1869.954812538044\n",
      "Iteration: 245 Loss: 1869.2438684209194\n",
      "Iteration: 246 Loss: 1868.5332038948118\n",
      "Iteration: 247 Loss: 1867.8228188497674\n",
      "Iteration: 248 Loss: 1867.1127131758756\n",
      "Iteration: 249 Loss: 1866.402886763268\n",
      "Iteration: 250 Loss: 1865.6933395021217\n",
      "Iteration: 251 Loss: 1864.9840712826538\n",
      "Iteration: 252 Loss: 1864.275081995128\n",
      "Iteration: 253 Loss: 1863.5663715298488\n",
      "Iteration: 254 Loss: 1862.8579397771646\n",
      "Iteration: 255 Loss: 1862.1497866274667\n",
      "Iteration: 256 Loss: 1861.4419119711902\n",
      "Iteration: 257 Loss: 1860.734315698812\n",
      "Iteration: 258 Loss: 1860.0269977008536\n",
      "Iteration: 259 Loss: 1859.3199578678784\n",
      "Iteration: 260 Loss: 1858.6131960904931\n",
      "Iteration: 261 Loss: 1857.9067122593478\n",
      "Iteration: 262 Loss: 1857.2005062651353\n",
      "Iteration: 263 Loss: 1856.4945779985912\n",
      "Iteration: 264 Loss: 1855.7889273504948\n",
      "Iteration: 265 Loss: 1855.0835542116677\n",
      "Iteration: 266 Loss: 1854.3784584729744\n",
      "Iteration: 267 Loss: 1853.6736400253224\n",
      "Iteration: 268 Loss: 1852.969098759663\n",
      "Iteration: 269 Loss: 1852.2648345669888\n",
      "Iteration: 270 Loss: 1851.5608473383359\n",
      "Iteration: 271 Loss: 1850.857136964784\n",
      "Iteration: 272 Loss: 1850.1537033374555\n",
      "Iteration: 273 Loss: 1849.450546347514\n",
      "Iteration: 274 Loss: 1848.7476658861679\n",
      "Iteration: 275 Loss: 1848.045061844667\n",
      "Iteration: 276 Loss: 1847.3427341143056\n",
      "Iteration: 277 Loss: 1846.6406825864185\n",
      "Iteration: 278 Loss: 1845.9389071523842\n",
      "Iteration: 279 Loss: 1845.2374077036247\n",
      "Iteration: 280 Loss: 1844.5361841316035\n",
      "Iteration: 281 Loss: 1843.8352363278284\n",
      "Iteration: 282 Loss: 1843.1345641838468\n",
      "Iteration: 283 Loss: 1842.434167591253\n",
      "Iteration: 284 Loss: 1841.7340464416802\n",
      "Iteration: 285 Loss: 1841.0342006268063\n",
      "Iteration: 286 Loss: 1840.3346300383503\n",
      "Iteration: 287 Loss: 1839.635334568076\n",
      "Iteration: 288 Loss: 1838.9363141077877\n",
      "Iteration: 289 Loss: 1838.2375685493325\n",
      "Iteration: 290 Loss: 1837.5390977846016\n",
      "Iteration: 291 Loss: 1836.8409017055271\n",
      "Iteration: 292 Loss: 1836.1429802040843\n",
      "Iteration: 293 Loss: 1835.4453331722902\n",
      "Iteration: 294 Loss: 1834.7479605022056\n",
      "Iteration: 295 Loss: 1834.0508620859332\n",
      "Iteration: 296 Loss: 1833.3540378156172\n",
      "Iteration: 297 Loss: 1832.6574875834453\n",
      "Iteration: 298 Loss: 1831.961211281647\n",
      "Iteration: 299 Loss: 1831.2652088024954\n",
      "Iteration: 300 Loss: 1830.5694800383044\n",
      "Iteration: 301 Loss: 1829.8740248814304\n",
      "Iteration: 302 Loss: 1829.1788432242738\n",
      "Iteration: 303 Loss: 1828.4839349592746\n",
      "Iteration: 304 Loss: 1827.7892999789185\n",
      "Iteration: 305 Loss: 1827.0949381757298\n",
      "Iteration: 306 Loss: 1826.4008494422776\n",
      "Iteration: 307 Loss: 1825.707033671173\n",
      "Iteration: 308 Loss: 1825.0134907550678\n",
      "Iteration: 309 Loss: 1824.3202205866583\n",
      "Iteration: 310 Loss: 1823.6272230586812\n",
      "Iteration: 311 Loss: 1822.934498063915\n",
      "Iteration: 312 Loss: 1822.2420454951832\n",
      "Iteration: 313 Loss: 1821.549865245348\n",
      "Iteration: 314 Loss: 1820.8579572073165\n",
      "Iteration: 315 Loss: 1820.1663212740357\n",
      "Iteration: 316 Loss: 1819.4749573384966\n",
      "Iteration: 317 Loss: 1818.7838652937303\n",
      "Iteration: 318 Loss: 1818.0930450328124\n",
      "Iteration: 319 Loss: 1817.402496448858\n",
      "Iteration: 320 Loss: 1816.7122194350266\n",
      "Iteration: 321 Loss: 1816.0222138845177\n",
      "Iteration: 322 Loss: 1815.3324796905736\n",
      "Iteration: 323 Loss: 1814.6430167464794\n",
      "Iteration: 324 Loss: 1813.9538249455609\n",
      "Iteration: 325 Loss: 1813.2649041811867\n",
      "Iteration: 326 Loss: 1812.5762543467663\n",
      "Iteration: 327 Loss: 1811.887875335753\n",
      "Iteration: 328 Loss: 1811.19976704164\n",
      "Iteration: 329 Loss: 1810.5119293579633\n",
      "Iteration: 330 Loss: 1809.8243621783006\n",
      "Iteration: 331 Loss: 1809.137065396272\n",
      "Iteration: 332 Loss: 1808.4500389055388\n",
      "Iteration: 333 Loss: 1807.7632825998041\n",
      "Iteration: 334 Loss: 1807.076796372813\n",
      "Iteration: 335 Loss: 1806.390580118352\n",
      "Iteration: 336 Loss: 1805.7046337302515\n",
      "Iteration: 337 Loss: 1805.0189571023795\n",
      "Iteration: 338 Loss: 1804.33355012865\n",
      "Iteration: 339 Loss: 1803.6484127030158\n",
      "Iteration: 340 Loss: 1802.9635447194728\n",
      "Iteration: 341 Loss: 1802.2789460720583\n",
      "Iteration: 342 Loss: 1801.594616654851\n",
      "Iteration: 343 Loss: 1800.910556361972\n",
      "Iteration: 344 Loss: 1800.2267650875835\n",
      "Iteration: 345 Loss: 1799.5432427258886\n",
      "Iteration: 346 Loss: 1798.859989171133\n",
      "Iteration: 347 Loss: 1798.177004317604\n",
      "Iteration: 348 Loss: 1797.4942880596307\n",
      "Iteration: 349 Loss: 1796.8118402915827\n",
      "Iteration: 350 Loss: 1796.1296609078715\n",
      "Iteration: 351 Loss: 1795.44774980295\n",
      "Iteration: 352 Loss: 1794.7661068713146\n",
      "Iteration: 353 Loss: 1794.0847320074997\n",
      "Iteration: 354 Loss: 1793.403625106084\n",
      "Iteration: 355 Loss: 1792.7227860616872\n",
      "Iteration: 356 Loss: 1792.0422147689678\n",
      "Iteration: 357 Loss: 1791.3619111226305\n",
      "Iteration: 358 Loss: 1790.6818750174168\n",
      "Iteration: 359 Loss: 1790.002106348112\n",
      "Iteration: 360 Loss: 1789.3226050095425\n",
      "Iteration: 361 Loss: 1788.6433708965765\n",
      "Iteration: 362 Loss: 1787.9644039041223\n",
      "Iteration: 363 Loss: 1787.2857039271298\n",
      "Iteration: 364 Loss: 1786.6072708605911\n",
      "Iteration: 365 Loss: 1785.9291045995394\n",
      "Iteration: 366 Loss: 1785.2512050390474\n",
      "Iteration: 367 Loss: 1784.5735720742327\n",
      "Iteration: 368 Loss: 1783.89620560025\n",
      "Iteration: 369 Loss: 1783.2191055122983\n",
      "Iteration: 370 Loss: 1782.542271705616\n",
      "Iteration: 371 Loss: 1781.8657040754842\n",
      "Iteration: 372 Loss: 1781.1894025172235\n",
      "Iteration: 373 Loss: 1780.513366926197\n",
      "Iteration: 374 Loss: 1779.8375971978094\n",
      "Iteration: 375 Loss: 1779.162093227504\n",
      "Iteration: 376 Loss: 1778.4868549107678\n",
      "Iteration: 377 Loss: 1777.8118821431274\n",
      "Iteration: 378 Loss: 1777.1371748201516\n",
      "Iteration: 379 Loss: 1776.4627328374495\n",
      "Iteration: 380 Loss: 1775.788556090672\n",
      "Iteration: 381 Loss: 1775.1146444755095\n",
      "Iteration: 382 Loss: 1774.4409978876952\n",
      "Iteration: 383 Loss: 1773.767616223002\n",
      "Iteration: 384 Loss: 1773.0944993772446\n",
      "Iteration: 385 Loss: 1772.4216472462788\n",
      "Iteration: 386 Loss: 1771.7490597260005\n",
      "Iteration: 387 Loss: 1771.0767367123465\n",
      "Iteration: 388 Loss: 1770.4046781012955\n",
      "Iteration: 389 Loss: 1769.732883788867\n",
      "Iteration: 390 Loss: 1769.0613536711203\n",
      "Iteration: 391 Loss: 1768.3900876441573\n",
      "Iteration: 392 Loss: 1767.7190856041186\n",
      "Iteration: 393 Loss: 1767.0483474471876\n",
      "Iteration: 394 Loss: 1766.3778730695872\n",
      "Iteration: 395 Loss: 1765.707662367582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 396 Loss: 1765.037715237477\n",
      "Iteration: 397 Loss: 1764.3680315756176\n",
      "Iteration: 398 Loss: 1763.6986112783907\n",
      "Iteration: 399 Loss: 1763.0294542422237\n",
      "Iteration: 400 Loss: 1762.3605603635847\n",
      "Iteration: 401 Loss: 1761.6919295389823\n",
      "Iteration: 402 Loss: 1761.0235616649659\n",
      "Iteration: 403 Loss: 1760.3554566381256\n",
      "Iteration: 404 Loss: 1759.6876143550926\n",
      "Iteration: 405 Loss: 1759.020034712538\n",
      "Iteration: 406 Loss: 1758.3527176071746\n",
      "Iteration: 407 Loss: 1757.6856629357544\n",
      "Iteration: 408 Loss: 1757.0188705950704\n",
      "Iteration: 409 Loss: 1756.3523404819575\n",
      "Iteration: 410 Loss: 1755.6860724932892\n",
      "Iteration: 411 Loss: 1755.0200665259817\n",
      "Iteration: 412 Loss: 1754.3543224769896\n",
      "Iteration: 413 Loss: 1753.6888402433096\n",
      "Iteration: 414 Loss: 1753.0236197219785\n",
      "Iteration: 415 Loss: 1752.358660810072\n",
      "Iteration: 416 Loss: 1751.6939634047094\n",
      "Iteration: 417 Loss: 1751.0295274030475\n",
      "Iteration: 418 Loss: 1750.3653527022861\n",
      "Iteration: 419 Loss: 1749.7014391996627\n",
      "Iteration: 420 Loss: 1749.0377867924578\n",
      "Iteration: 421 Loss: 1748.3743953779908\n",
      "Iteration: 422 Loss: 1747.711264853621\n",
      "Iteration: 423 Loss: 1747.0483951167496\n",
      "Iteration: 424 Loss: 1746.3857860648175\n",
      "Iteration: 425 Loss: 1745.7234375953055\n",
      "Iteration: 426 Loss: 1745.0613496057354\n",
      "Iteration: 427 Loss: 1744.3995219936692\n",
      "Iteration: 428 Loss: 1743.737954656708\n",
      "Iteration: 429 Loss: 1743.0766474924953\n",
      "Iteration: 430 Loss: 1742.4156003987134\n",
      "Iteration: 431 Loss: 1741.7548132730838\n",
      "Iteration: 432 Loss: 1741.0942860133712\n",
      "Iteration: 433 Loss: 1740.4340185173783\n",
      "Iteration: 434 Loss: 1739.7740106829488\n",
      "Iteration: 435 Loss: 1739.1142624079653\n",
      "Iteration: 436 Loss: 1738.4547735903525\n",
      "Iteration: 437 Loss: 1737.795544128075\n",
      "Iteration: 438 Loss: 1737.136573919135\n",
      "Iteration: 439 Loss: 1736.477862861578\n",
      "Iteration: 440 Loss: 1735.8194108534874\n",
      "Iteration: 441 Loss: 1735.161217792989\n",
      "Iteration: 442 Loss: 1734.5032835782451\n",
      "Iteration: 443 Loss: 1733.845608107462\n",
      "Iteration: 444 Loss: 1733.1881912788833\n",
      "Iteration: 445 Loss: 1732.5310329907934\n",
      "Iteration: 446 Loss: 1731.8741331415174\n",
      "Iteration: 447 Loss: 1731.217491629419\n",
      "Iteration: 448 Loss: 1730.5611083529036\n",
      "Iteration: 449 Loss: 1729.9049832104147\n",
      "Iteration: 450 Loss: 1729.249116100437\n",
      "Iteration: 451 Loss: 1728.5935069214956\n",
      "Iteration: 452 Loss: 1727.9381555721534\n",
      "Iteration: 453 Loss: 1727.2830619510148\n",
      "Iteration: 454 Loss: 1726.6282259567242\n",
      "Iteration: 455 Loss: 1725.9736474879649\n",
      "Iteration: 456 Loss: 1725.3193264434606\n",
      "Iteration: 457 Loss: 1724.6652627219753\n",
      "Iteration: 458 Loss: 1724.011456222312\n",
      "Iteration: 459 Loss: 1723.3579068433135\n",
      "Iteration: 460 Loss: 1722.7046144838628\n",
      "Iteration: 461 Loss: 1722.0515790428828\n",
      "Iteration: 462 Loss: 1721.3988004193359\n",
      "Iteration: 463 Loss: 1720.746278512224\n",
      "Iteration: 464 Loss: 1720.0940132205887\n",
      "Iteration: 465 Loss: 1719.4420044435126\n",
      "Iteration: 466 Loss: 1718.7902520801151\n",
      "Iteration: 467 Loss: 1718.1387560295584\n",
      "Iteration: 468 Loss: 1717.4875161910434\n",
      "Iteration: 469 Loss: 1716.8365324638096\n",
      "Iteration: 470 Loss: 1716.1858047471371\n",
      "Iteration: 471 Loss: 1715.5353329403447\n",
      "Iteration: 472 Loss: 1714.8851169427921\n",
      "Iteration: 473 Loss: 1714.2351566538775\n",
      "Iteration: 474 Loss: 1713.5854519730399\n",
      "Iteration: 475 Loss: 1712.9360027997561\n",
      "Iteration: 476 Loss: 1712.286809033544\n",
      "Iteration: 477 Loss: 1711.6378705739596\n",
      "Iteration: 478 Loss: 1710.9891873206\n",
      "Iteration: 479 Loss: 1710.3407591731002\n",
      "Iteration: 480 Loss: 1709.6925860311362\n",
      "Iteration: 481 Loss: 1709.0446677944221\n",
      "Iteration: 482 Loss: 1708.3970043627126\n",
      "Iteration: 483 Loss: 1707.7495956358002\n",
      "Iteration: 484 Loss: 1707.102441513519\n",
      "Iteration: 485 Loss: 1706.4555418957407\n",
      "Iteration: 486 Loss: 1705.8088966823775\n",
      "Iteration: 487 Loss: 1705.16250577338\n",
      "Iteration: 488 Loss: 1704.5163690687386\n",
      "Iteration: 489 Loss: 1703.8704864684835\n",
      "Iteration: 490 Loss: 1703.224857872683\n",
      "Iteration: 491 Loss: 1702.5794831814464\n",
      "Iteration: 492 Loss: 1701.9343622949216\n",
      "Iteration: 493 Loss: 1701.2894951132944\n",
      "Iteration: 494 Loss: 1700.6448815367912\n",
      "Iteration: 495 Loss: 1700.000521465678\n",
      "Iteration: 496 Loss: 1699.356414800259\n",
      "Iteration: 497 Loss: 1698.712561440878\n",
      "Iteration: 498 Loss: 1698.0689612879182\n",
      "Iteration: 499 Loss: 1697.4256142418024\n",
      "Iteration: 500 Loss: 1696.7825202029908\n"
     ]
    }
   ],
   "source": [
    "predicted = predict_weighted_linear(X_train, y_train, x_pred, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оцінимо, наскільки модель помиляється на тестовій вибірці:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = np.array([predict_weighted_linear(X_test, y_test, X_test[i]) for i in range(len(y_test))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_residuals = pd.DataFrame(y_test - y_test_pred, columns=[\"residual\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 1.0, 'Error Distribution')]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = df_residuals.plot.bar(figsize=(10, 5))\n",
    "ax.legend().remove()\n",
    "ax.get_xaxis().set(ticklabels=[])\n",
    "ax.set(xlabel=\"Example #\")\n",
    "ax.set(ylabel=\"Prediction Error, $\")\n",
    "ax.set(title=\"Residuals\")\n",
    "\n",
    "bins = np.arange(-3000, 3001, 100)\n",
    "ticks = np.arange(-3000, 3001, 500)\n",
    "\n",
    "ax = df_residuals.hist(bins=bins, figsize=(10, 5))[0][0]\n",
    "ax.set(xticks=ticks)\n",
    "ax.set(xlabel=\"Prediction Error, $\")\n",
    "ax.set(ylabel=\"Frequency\")\n",
    "ax.set(title=\"Error Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестування"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустіть комірку нижче, щоб перевірити правильність вашого коду:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/unittest.status+json": {
       "color": "yellow",
       "message": "",
       "previous": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/unittest.status+json": {
       "color": "lightgreen",
       "message": "............\n----------------------------------------------------------------------\nRan 12 tests in 0.001s\n\nOK\n",
       "previous": 0
      },
      "text/plain": [
       "Success"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............\n",
      "----------------------------------------------------------------------\n",
      "Ran 12 tests in 0.001s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=12 errors=0 failures=0>"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%unittest_main\n",
    "\n",
    "class WLRTests(unittest.TestCase):\n",
    "\n",
    "    x = np.array([1, -0.5, 3, 1])\n",
    "    X = np.array([\n",
    "        [1, -0.5, 3, 1],\n",
    "        [2, 8, -0.33, 5],\n",
    "        [0, 0, 0, 0]\n",
    "    ])\n",
    "    y = np.array([40, 100, 12])\n",
    "    theta = np.array([2, 5, 7, 9])\n",
    "    eps = 0.001\n",
    "\n",
    "    def assertFloatEquals(self, a, b):\n",
    "        self.assertTrue(np.abs(a - b) < self.eps)\n",
    "    \n",
    "    def assertArrayEquals(self, a, b):\n",
    "        a = np.array(a)\n",
    "        b = np.array(b)\n",
    "        self.assertEqual(a.shape, b.shape)\n",
    "        self.assertTrue(np.all(np.abs(a - b) < self.eps))\n",
    "    \n",
    "    def test_predict_linear_should_compute_correct_prediction_for_1_example(self):\n",
    "        expected = 29.5\n",
    "        actual = predict_linear(self.theta, self.x)\n",
    "        self.assertEqual(actual, expected)\n",
    "    \n",
    "    def test_predict_linear_should_compute_correct_predictions_for_multiple_examples(self):\n",
    "        expected = [29.5, 86.69, 0]\n",
    "        actual = (predict_linear(self.theta, self.X))\n",
    "        self.assertArrayEquals(actual, expected)\n",
    "\n",
    "    def test_get_example_weights_should_return_properly_shaped_vector(self):\n",
    "        weights = get_example_weights(self.X, self.x, tau=5)\n",
    "        self.assertTrue(weights.shape[0] == self.X.shape[0])\n",
    "\n",
    "    def test_get_example_weights_should_compute_correct_weights(self):\n",
    "        expected = [1.000, 0.134, 0.798]\n",
    "        actual = get_example_weights(self.X, self.x, tau=5)\n",
    "        self.assertArrayEquals(actual, expected)\n",
    "               \n",
    "    def test_cost_function_should_compute_correct_cost_unweighted(self):\n",
    "        weights = np.ones(self.X.shape[0])\n",
    "        expected = 71.901\n",
    "        actual = cost_function(self.theta, self.X, self.y, weights)\n",
    "        self.assertFloatEquals(actual, expected) \n",
    "\n",
    "    def test_cost_function_should_compute_correct_cost_weighted(self):\n",
    "        weights = np.array([0.5, 0.1, 0.28])\n",
    "        expected = 18.860\n",
    "        actual = cost_function(self.theta, self.X, self.y, weights)\n",
    "        self.assertFloatEquals(actual, expected)\n",
    "\n",
    "    def test_cost_gradient_should_return_properly_shaped_vector(self):\n",
    "        weights = np.ones(self.X.shape[0])\n",
    "        grad = cost_function_gradient(self.theta, self.X, self.y, weights)\n",
    "        self.assertTrue(grad.shape == self.theta.shape)\n",
    "        \n",
    "    def test_cost_gradient_should_compute_correct_gradient_unweighted(self):\n",
    "        weights = np.ones(self.X.shape[0])\n",
    "        expected = [-12.373, -33.743, -9.036, -25.683]\n",
    "        actual = cost_function_gradient(self.theta, self.X, self.y, weights)\n",
    "        self.assertTrue(np.all(np.abs(actual - expected) < 1e-2))\n",
    "    \n",
    "    def test_cost_gradient_should_compute_correct_gradient_weighted(self):\n",
    "        weights = np.array([0.5, 0.1, 0.28])\n",
    "        expected = [-2.637, -2.674, -5.103, -3.968]\n",
    "        actual = cost_function_gradient(self.theta, self.X, self.y, weights)\n",
    "        self.assertTrue(np.all(np.abs(actual - expected) < 1e-2))\n",
    "        \n",
    "    def test_update_model_weights_should_not_update_when_gradient_is_zero(self):\n",
    "        grad = np.zeros(self.theta.shape[0])\n",
    "        theta_new = update_model_weights(self.theta, learning_rate=1, cost_gradient=grad)\n",
    "        self.assertArrayEquals(theta_new, self.theta)\n",
    "    \n",
    "    def test_update_model_weights_should_update_with_complete_gradient_if_learning_rate_is_one(self):\n",
    "        grad = np.array([1.35, -0.89, 0.16, 0.98])\n",
    "        expected = [0.65, 5.89, 6.84, 8.02]\n",
    "        actual = update_model_weights(self.theta, learning_rate=1, cost_gradient=grad)\n",
    "        self.assertArrayEquals(actual, expected)\n",
    "    \n",
    "    def test_update_model_weights_should_take_learning_rate_into_account(self):\n",
    "        grad = np.array([1.35, -0.89, 0.16, 0.98])\n",
    "        expected = [1.730, 5.178, 6.968, 8.804]\n",
    "        actual = update_model_weights(self.theta, learning_rate=0.2, cost_gradient=grad)\n",
    "        self.assertArrayEquals(actual, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
